{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKoWcn-o6V3j"
      },
      "source": [
        "#**Heart Disease Detection System Using Decision Tree Classifier**\n",
        "\n",
        "\n",
        "> This is a post-workshop assignment for Google Developer Student Club ITB <br>\n",
        "Rizky Ramadhana P. K.<br>\n",
        "16520285<br>\n",
        "Institut Teknologi Bandung<br>\n",
        "8 November 2020<br>\n",
        "Dataset Source : https://archive.ics.uci.edu/ml/datasets/heart+Disease\n",
        "\n",
        "Every year, 17.9 million people die due to hearth disease. This contributes to an estimated 31% of all deaths worldwide. Furthermore, 85% of all heart disease deaths are caused by heart attack and strokes, which usually called 'silent killer'*. Hence, it is very important to create a heart disease early detection system to reduce the number of heart disease death. <br>\n",
        "Here, we will utilize data from heart disease patient and process it with machine learning algorithm. The main purpose is creting a model that can predict whether someone have a risk at heart disease or not based on several feature. Then, we deploy our trained machine learning alhorithm to a website so that everybody could use them.<br><br>\n",
        "*cited from [WHO's article on cardiovascular disease](https://www.who.int/health-topics/cardiovascular-diseases/) <br><br>\n",
        "## Data Exploration And Data Preprocessing\n",
        "We will use dataset from [University of California Irvine's Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/heart+Disease), which is oftenly used by researchers across the globe. Without further ado, let's go to the first step, explore the data. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzkl1-DHfNn9"
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('processed.cleveland.csv', header = None)\n",
        "data.columns = ['age', 'is_male', 'chestpain', 'restbps', 'chol', 'fbs', 'restecg', 'mhr', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
        "#columns' name are listed on dataset's link. Detailed feature for each column would be explained soon\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvDDInpt6ScL"
      },
      "source": [
        "When we observe the data, we know that some rows have a not valid value. It use '?' instead of numbers. I choose to remove those rows since our data is big enough and would not lose its coherence. Then, we convert all value in this data to numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JYSaPUCEafm"
      },
      "source": [
        "for n in data.columns:            #Remove rows with '?'\n",
        "    data = data[~(data[n]=='?')]\n",
        "\n",
        "data = data.apply(pd.to_numeric)  #Convert to numbers\n",
        "data['target'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Tt5jyM3Ewzx"
      },
      "source": [
        "The column 'target' will have an integer value from 0 (no heart disease occurs) to 4. We will find out that column 'target' is imbalanced since every value does not have same quantity.\n",
        "\n",
        "\n",
        "    Quantity for each value in 'target'\n",
        "    0    160\n",
        "    1     54\n",
        "    3     35\n",
        "    2     35\n",
        "    4     13\n",
        "\n",
        "So, I decided to group the data into only two target. Zero for absence of heart disease and one for presence.<br>\n",
        "I also decided to only use several column since I want this model would work for everyone without fancy blood test result. Here are the column that we will use to build the model:\n",
        "\n",
        "\n",
        "1. Age\n",
        "2. Sex\n",
        "3. Presence of Angina (chest pain)*. Here are the values :\n",
        "\n",
        "  * 1 for typical angina\n",
        "  * 2 for atypical angina\n",
        "  * 3 for non-anginal pain\n",
        "  * 4 for asymptomatic <br>\n",
        "  A pain will be categorized as typical angina if it meets this three criteria :\n",
        "  * Substernal chest discomfort of characteristic quality and duration\n",
        "  * Provoked by exertion or emotional stress\n",
        "  * Relieved by rest and/or nitroglycerine <br>\n",
        "  An atypical angina will meets two of three criteria. A non-anginal pain will only meet one criteria. While asymptomatic means none of that criteria are experinced by the user.\n",
        "\n",
        "4. Resting Blood Pressure (in mmHg)\n",
        "5. Serum Cholesterol Level (in mg/dL)\n",
        "6. Presence of Exercise Induced Angina <br>\n",
        "Will have value of one if the user are experiencing chest pain while doing hard activities. <br><br>\n",
        "*cited from [TextBookofCardiology.org](https://www.textbookofcardiology.org/wiki/Chest_Pain_/_Angina_Pectoris)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwGlttT_LueO"
      },
      "source": [
        "data.loc[(data['target']>0), 'target'] = 1                      #Group the data to only two target, presence and absence of heart disease\n",
        "data.drop(['fbs', 'restecg', 'oldpeak', 'slope', 'ca', 'thal'], inplace =True, axis = 1)       #Removing columns that we would not use\n",
        "\n",
        "data['is_male'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jFCXwlZLs9q"
      },
      "source": [
        "Next, we observe the column 'is_male', which has a value of one if the patient is male and value of zero if the patient is female. If we look closely, the numbers of male and female are not balanced. <br>\n",
        "\n",
        "\n",
        "\n",
        "> \n",
        "\n",
        "\n",
        "    Numbers of male and female\n",
        "    1.0    201\n",
        "    0.0     96\n",
        "\n",
        "Hence, I decided to upsample the female data so that it will create a balanced and good data for our machine learning model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwdglVSUO3ze"
      },
      "source": [
        "from sklearn.utils import resample                  #Upsample the female data\n",
        "female = data[data['is_male']==0]\n",
        "male = data[data['is_male']==1]\n",
        "upsampled = resample(female, replace=True, n_samples = 201, random_state = 77)\n",
        "data = pd.concat([upsampled, male])\n",
        "\n",
        "x = data.iloc[:, :7]       #Features of each patient (feature)\n",
        "y = data['target']         #List that told us whether the patient has a heart disease or not (target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c153PV8UVY4S"
      },
      "source": [
        "Here, we are finished in exploring and preprocessing our data. We hope that this clean data would train our model well. Below are how our data looks like after being cleaned.\n",
        "\n",
        "\n",
        "> \n",
        "          age  is_male  chestpain  restbps   chol    mhr  exang  target\n",
        "    0    63.0      1.0        1.0    145.0  233.0  150.0    0.0       0\n",
        "    1    67.0      1.0        4.0    160.0  286.0  108.0    1.0       1\n",
        "    2    67.0      1.0        4.0    120.0  229.0  129.0    1.0       1\n",
        "    3    37.0      1.0        3.0    130.0  250.0  187.0    0.0       0\n",
        "    4    41.0      0.0        2.0    130.0  204.0  172.0    0.0       0\n",
        "    ..    ...      ...        ...      ...    ...    ...    ...     ...\n",
        "    297  57.0      0.0        4.0    140.0  241.0  123.0    1.0       1\n",
        "    298  45.0      1.0        1.0    110.0  264.0  132.0    0.0       1\n",
        "    299  68.0      1.0        4.0    144.0  193.0  141.0    0.0       1\n",
        "    300  57.0      1.0        4.0    130.0  131.0  115.0    1.0       1\n",
        "    301  57.0      0.0        2.0    130.0  236.0  174.0    0.0       1    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-t50orRPfeu"
      },
      "source": [
        "## Choosing The Right Model\n",
        "We would try Decision Tree, K-Nearest Neighbors, Random Forest, Naive Bayes, and Logistic Regression. Then we will choose model with highest accuracy to be deployed later. We would use Cross Validation method to calculate accuracy. For Decision Tree, K-Nearest Neighbors, and Random Forest, we also tune the hyperparameter using GridSearchCV. The hyperparameter that would be tuned is stored in variable 'parameter'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbGfENvZRfOd"
      },
      "source": [
        "import sklearn                                                     #Defining the model\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "dtree = DecisionTreeClassifier(max_depth = 2)\n",
        "knn = KNeighborsClassifier(n_neighbors = 2)\n",
        "gnb = GaussianNB()\n",
        "randomforest = RandomForestClassifier(max_depth = 2)\n",
        "log = LogisticRegression(max_iter = 1000)\n",
        "\n",
        "parameter = {                                              #Hyperparameter that will be tuned\n",
        "    dtree : {'max_depth' : range(1,31)},\n",
        "    knn : {'n_neighbors' : range(1,31)},\n",
        "    randomforest : {'max_depth' : range(1,31)}\n",
        "}\n",
        "\n",
        "for i in [dtree, knn, randomforest]:                      #Finding the best hyperparameter for each model\n",
        "    gridsearch = GridSearchCV(i, param_grid = parameter[i], scoring = 'accuracy', cv = 5)\n",
        "    gridsearch.fit(x,y)\n",
        "    print(gridsearch.best_score_)                       #Print the best accuracy for each model\n",
        "    print(gridsearch.best_params_)                      #Print hyperparameter that yields best accuracy\n",
        "\n",
        "cross_val_score(gnb, x, y, scoring = 'accuracy', cv = 5).mean()   #Calculating accuracy for Naive Bayes model\n",
        "cross_val_score(log, x, y, scoring = 'accuracy', cv = 5).mean()   #Calculating accuracy for Logistic Regression model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0W_9kNYSpcv"
      },
      "source": [
        "When I ran the code, the best model was Decision Tree Classifier with hyperparameter 'max_depth' = 11. It yields accuracy of 81 %. Therefore, we would use this model and save it to .pkl file to be deployed on the website later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeG8MReETMFU"
      },
      "source": [
        "model = DecisionTreeClassifier(max_depth = 11)\n",
        "model.fit(x,y)\n",
        "import pickle\n",
        "pickle.dump(model, open('model.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYw-zfXPWWxT"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "\n",
        "1.   80+ % accuracy means that there are really a correlation between features that we choose and the presence of heart disease.\n",
        "2.   Since the accuracy is less than 90 %, I suggest that this model is not used in making medical desicion.\n",
        "3. However, we still can use this model as a early detection of heart disease. But, further examination should be done by doctor.\n",
        "\n"
      ]
    }
  ]
}